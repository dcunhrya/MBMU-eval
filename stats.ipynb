{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919172f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec2c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This eval script calculates CIs for all results we run:\n",
    "\n",
    "def calculate_confidence_interval(\n",
    "    data:np.array,\n",
    "    percentage:bool=True,\n",
    "    round_to:int=3,\n",
    "    method='t',\n",
    "    confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Calculate the mean, standard error, and confidence interval for a mean (Could be extended to any point estimate).\n",
    "\n",
    "    This function can compute the confidence interval using different statistical methods: t-score, z-score, or bootstrap.\n",
    "    The results can be expressed as either proportions or percentages, and the output can be rounded to a specified number\n",
    "    of decimal places.\n",
    "\n",
    "    Args:\n",
    "        is_correct (array-like): An array of binary values (0 or 1) indicating correctness.\n",
    "        percentage (bool, optional): If True, results are returned as percentages (0-100). Defaults to True.\n",
    "        round_to (int, optional): Number of decimal places to round the results to. Defaults to 3.\n",
    "        method (str, optional): Statistical method for confidence interval calculation ('t', 'z', or 'bootstrap'). Defaults to 't'.\n",
    "        confidence_level (float, optional): Confidence level for the interval, expressed as a decimal (e.g., 0.95 for 95% confidence). Defaults to 0.95.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the mean, standard error, lower bound of the confidence interval,\n",
    "               and upper bound of the confidence interval. The format depends on the `percentage` argument.\n",
    "    \"\"\"\n",
    "    # Calculate mean and standard error\n",
    "    mean      = data.mean()\n",
    "    std_error = data.std() / np.sqrt(len(data))\n",
    "\n",
    "    # Confidence interval calculation based on method\n",
    "    if method == 't':\n",
    "        # T-distribution\n",
    "        degrees_freedom = len(data) - 1\n",
    "        confidence_interval = stats.t.interval(confidence_level, degrees_freedom, mean, std_error)\n",
    "\n",
    "    elif method == 'z':\n",
    "        # Z-distribution\n",
    "        z_value = stats.norm.ppf(1 - (1 - confidence_level) / 2)  # Two-tailed z-value for the confidence level\n",
    "        confidence_interval = (mean - z_value * std_error, mean + z_value * std_error)\n",
    "\n",
    "    elif method == 'bootstrap':\n",
    "        # Bootstrap confidence interval\n",
    "        n_iterations = 1000\n",
    "        bootstrapped_means = []\n",
    "        for _ in range(n_iterations):\n",
    "            sample = stats.resample(data, replace=True)\n",
    "            bootstrapped_means.append(sample.mean())\n",
    "        lower = np.percentile(bootstrapped_means, (1 - confidence_level) / 2 * 100)\n",
    "        upper = np.percentile(bootstrapped_means, (1 + confidence_level) / 2 * 100)\n",
    "        confidence_interval = (lower, upper)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 't', 'z', or 'bootstrap'.\")\n",
    "\n",
    "    # Convert to percentage if required\n",
    "    if percentage:\n",
    "        mean *= 100\n",
    "        confidence_interval = [ci * 100 for ci in confidence_interval]\n",
    "\n",
    "    # Round results if specified\n",
    "    if round_to is not None:\n",
    "        mean = round(mean, round_to)\n",
    "        confidence_interval = [round(ci, round_to) for ci in confidence_interval]\n",
    "\n",
    "    out = {\"mean\":mean,\"ci\":confidence_interval}\n",
    "    return out\n",
    "\n",
    "\n",
    "def format_output(question_type:str,\n",
    "                  metrics:dict[str,float]) -> None:\n",
    "    \"\"\"\n",
    "    Print formatted output for the given question type and its associated metrics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question_type : str\n",
    "        The type of question being processed, which will be capitalized and displayed.\n",
    "    metrics : dict[str, float]\n",
    "        A dictionary containing metric information. It must include:\n",
    "        - 'mean': The mean accuracy to display.\n",
    "        - 'ci': A list or tuple containing two values representing the confidence interval.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function does not return any value; it simply prints the formatted output.\n",
    "    \"\"\"\n",
    "    print(f\"{question_type:<36}  Acc: {metrics['mean']:>6.3f} ({metrics['ci'][0]:>4.3f},{metrics['ci'][1]:>4.3f})\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
