{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919172f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec2c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This eval script calculates CIs for all results we run:\n",
    "\n",
    "def calculate_confidence_interval(\n",
    "    data:np.array,\n",
    "    percentage:bool=True,\n",
    "    round_to:int=3,\n",
    "    method='t',\n",
    "    confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Calculate the mean, standard error, and confidence interval for a mean (Could be extended to any point estimate).\n",
    "\n",
    "    This function can compute the confidence interval using different statistical methods: t-score, z-score, or bootstrap.\n",
    "    The results can be expressed as either proportions or percentages, and the output can be rounded to a specified number\n",
    "    of decimal places.\n",
    "\n",
    "    Args:\n",
    "        is_correct (array-like): An array of binary values (0 or 1) indicating correctness.\n",
    "        percentage (bool, optional): If True, results are returned as percentages (0-100). Defaults to True.\n",
    "        round_to (int, optional): Number of decimal places to round the results to. Defaults to 3.\n",
    "        method (str, optional): Statistical method for confidence interval calculation ('t', 'z', or 'bootstrap'). Defaults to 't'.\n",
    "        confidence_level (float, optional): Confidence level for the interval, expressed as a decimal (e.g., 0.95 for 95% confidence). Defaults to 0.95.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the mean, standard error, lower bound of the confidence interval,\n",
    "               and upper bound of the confidence interval. The format depends on the `percentage` argument.\n",
    "    \"\"\"\n",
    "    # Calculate mean and standard error\n",
    "    mean      = data.mean()\n",
    "    std_error = data.std() / np.sqrt(len(data))\n",
    "\n",
    "    # Confidence interval calculation based on method\n",
    "    if method == 't':\n",
    "        # T-distribution\n",
    "        degrees_freedom = len(data) - 1\n",
    "        confidence_interval = stats.t.interval(confidence_level, degrees_freedom, mean, std_error)\n",
    "\n",
    "    elif method == 'z':\n",
    "        # Z-distribution\n",
    "        z_value = stats.norm.ppf(1 - (1 - confidence_level) / 2)  # Two-tailed z-value for the confidence level\n",
    "        confidence_interval = (mean - z_value * std_error, mean + z_value * std_error)\n",
    "\n",
    "    elif method == 'bootstrap':\n",
    "        # Bootstrap confidence interval\n",
    "        n_iterations = 1000\n",
    "        bootstrapped_means = []\n",
    "        for _ in range(n_iterations):\n",
    "            sample = stats.resample(data, replace=True)\n",
    "            bootstrapped_means.append(sample.mean())\n",
    "        lower = np.percentile(bootstrapped_means, (1 - confidence_level) / 2 * 100)\n",
    "        upper = np.percentile(bootstrapped_means, (1 + confidence_level) / 2 * 100)\n",
    "        confidence_interval = (lower, upper)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 't', 'z', or 'bootstrap'.\")\n",
    "\n",
    "    # Convert to percentage if required\n",
    "    if percentage:\n",
    "        mean *= 100\n",
    "        confidence_interval = [ci * 100 for ci in confidence_interval]\n",
    "\n",
    "    # Round results if specified\n",
    "    if round_to is not None:\n",
    "        mean = round(mean, round_to)\n",
    "        confidence_interval = [round(ci, round_to) for ci in confidence_interval]\n",
    "\n",
    "    out = {\"mean\":mean,\"ci\":confidence_interval}\n",
    "    return out\n",
    "\n",
    "\n",
    "def format_output(question_type:str,\n",
    "                  metrics:dict[str,float]) -> None:\n",
    "    \"\"\"\n",
    "    Print formatted output for the given question type and its associated metrics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question_type : str\n",
    "        The type of question being processed, which will be capitalized and displayed.\n",
    "    metrics : dict[str, float]\n",
    "        A dictionary containing metric information. It must include:\n",
    "        - 'mean': The mean accuracy to display.\n",
    "        - 'ci': A list or tuple containing two values representing the confidence interval.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function does not return any value; it simply prints the formatted output.\n",
    "    \"\"\"\n",
    "    print(f\"{question_type:<36}  Acc: {metrics['mean']:>6.3f} ({metrics['ci'][0]:>4.3f},{metrics['ci'][1]:>4.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f37140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pandas as pd, string, torch, re\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "\n",
    "def extract_letter(text, n_opts):\n",
    "    text = (text or \"\").strip().upper()\n",
    "    if len(text) == 1 and \"A\" <= text <= chr(ord(\"A\")+n_opts-1):\n",
    "        return text\n",
    "    m = re.search(r\"\\b([A-{}])\\b\".format(chr(ord(\"A\")+n_opts-1)), text)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "tsv = \"~/LMUData/uBench/uBench_classification_10.tsv\"\n",
    "row = pd.read_csv(tsv, sep=\"\\t\").iloc[0]\n",
    "opts = json.loads(row[\"options\"])\n",
    "letters = list(string.ascii_uppercase[:len(opts)])\n",
    "options_block = \"\\n\".join(f\"{letters[i]}. {opts[i]}\" for i in range(len(opts)))\n",
    "\n",
    "prompt = (\n",
    "    f\"{row['question'].strip()}\\n\"\n",
    "    f\"Options:\\n{options_block}\\n\"\n",
    "    f\"Answer with a single letter ({letters[0]}â€“{letters[-1]}) only.\"\n",
    ")\n",
    "\n",
    "image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "\n",
    "model_id = \"google/medgemma-4b-it\"  # or your current HF model\n",
    "proc = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "inputs = proc(images=image, text=prompt, return_tensors=\"pt\")\n",
    "# keep on CPU if model is sharded; else move to model device\n",
    "if getattr(model, \"hf_device_map\", None) is None:\n",
    "    dev = next(model.parameters()).device\n",
    "    inputs = {k:(v.to(dev)) for k,v in inputs.items()}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out_ids = model.generate(**inputs, max_new_tokens=2, do_sample=False)\n",
    "text = proc.decode(out_ids[0], skip_special_tokens=True)\n",
    "pred = extract_letter(text, len(opts))\n",
    "gt = row[\"answer\"]\n",
    "\n",
    "print(\"RAW OUT:\", repr(text))\n",
    "print(\"PRED:\", pred, \"GT:\", gt, \"->\", \"OK\" if pred==gt else \"WRONG\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
